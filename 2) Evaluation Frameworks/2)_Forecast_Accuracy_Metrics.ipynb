{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2) Forecast Accuracy Metrics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHgTNQh__c6b"
      },
      "source": [
        "# Evaluation Frameworks: Common Forecast Accuracy Metrics\n",
        "\n",
        "The goal of any time series forecasting model is to make accurate forecasts, but the question is how we can measure and compare the predictive accuracy. Therefore, as a preliminary requirement, we have to define a suitable performance metrics that measure predictive accuracy.\n",
        "\n",
        "The commonly used accuracy metrics to judge forecasts are:\n",
        "\n",
        "- Forecast Error\n",
        "- Mean Absolute Percentage Error (MAPE)\n",
        "- Mean (Forecast) Error (ME)\n",
        "- Mean Absolute Error (MAE)\n",
        "- Mean Percentage Error (MPE)\n",
        "- Mean Squared Error (MSE)\n",
        "- Root Mean Squared Error (RMSE)\n",
        "- Lag 1 Autocorrelation of Error (ACF1)\n",
        "- Correlation between the Actual and the Forecast (corr)\n",
        "- Min-Max Error (minmax)\n",
        "\n",
        "Typically, if you are comparing forecasts of two different series, the MAPE, Correlation and Min-Max Error can be used. Why not use the other metrics? Because only the above three are percentage errors that vary between 0 and 1. That way, you can judge how good is the forecast irrespective of the scale of the series.The other error metrics are quantities. That implies, an RMSE of 100 for a series whose mean is in 1000’s is better than an RMSE of 5 for series in 10’s. So, you can’t really use them to compare the forecasts of two different scaled time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ezpW2bUDFEq"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NftfEf58_DpS",
        "outputId": "791c2b11-5e1b-45a1-e2a5-1474ab1b9aca"
      },
      "source": [
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import acf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roi_Zh0eDNUQ"
      },
      "source": [
        "## Forecast Error\n",
        "\n",
        "Consider a validation dataset with ‘v’ periods, t=1,…v. The forecast error (e) is defined as the difference between an observed value (x)and the predicted value (y) at a time-period (t).\n",
        "\n",
        "$e(t)=y(t)-x(t)$\n",
        "\n",
        "The forecast error can be calculated for each prediction. Does this solve our problem of evaluation? Not yet, we have to summarize these errors as a single comparable number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYNWwRIZDcD3",
        "outputId": "8c281df7-8739-401f-e58d-aacff856923b"
      },
      "source": [
        "# Generate a dummy data and calculate forecast error\n",
        "actual = np.array([1.1, 1.5, 2.2, 2.9, 3.0,3.6,4.0,4.4,5.0,5.5])\n",
        "predicted = np.array([1.2,1.4,2.1,2.6,3.2,3.4,4.2,4.5,4.8,5.1])\n",
        "forecast_errors = [actual[i]-predicted[i] for i in range(len(actual))]\n",
        "print('Forecast Errors: %s' % forecast_errors)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Forecast Errors: [-0.09999999999999987, 0.10000000000000009, 0.10000000000000009, 0.2999999999999998, -0.20000000000000018, 0.20000000000000018, -0.20000000000000018, -0.09999999999999964, 0.20000000000000018, 0.40000000000000036]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4epbO4NDrag"
      },
      "source": [
        "## Others "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLn-5GqCDhF5"
      },
      "source": [
        "# Accuracy metrics\n",
        "def forecast_accuracy(forecast, actual):\n",
        "\n",
        "  # Mean Absolute Percentage Error (MAPE)\n",
        "  mape = np.mean(np.abs(forecast - actual)/np.abs(actual))\n",
        "\n",
        "  # Mean (Forecast) Error (ME)\n",
        "  me = np.mean(forecast - actual)       \n",
        "\n",
        "  # Mean Absolute Error (MAE)\n",
        "  mae = np.mean(np.abs(forecast - actual)) \n",
        "\n",
        "  # Mean Percentage Error (MPE)\n",
        "  mpe = np.mean((forecast - actual)/actual) \n",
        "\n",
        "  # Mean Squared Error (MSE)\n",
        "  mse = np.mean((forecast - actual)**2)\n",
        "\n",
        "  # Root Mean Squared Error (RMSE)\n",
        "  rmse = np.mean((forecast - actual)**2)**.5\n",
        "\n",
        "  # Correlation between the Actual and the Forecast (corr)\n",
        "  corr = np.corrcoef(forecast, actual)[0,1] \n",
        "\n",
        "  # Lag 1 Autocorrelation of Error (ACF1)\n",
        "  acf1 = acf(forecast-actual)[1]       \n",
        "\n",
        "  # Min-Max Error (minmax)\n",
        "  mins = np.amin(np.hstack([forecast[:,None], \n",
        "                            actual[:,None]]), axis=1)\n",
        "  maxs = np.amax(np.hstack([forecast[:,None], \n",
        "                            actual[:,None]]), axis=1)\n",
        "  minmax = 1 - np.mean(mins/maxs)      \n",
        "  \n",
        "  # output\n",
        "  return({'mape':mape,\n",
        "          'me':me, \n",
        "          'mae': mae, \n",
        "          'mpe': mpe, \n",
        "          'mse': mse,\n",
        "          'rmse':rmse, \n",
        "          'acf1':acf1, \n",
        "          'corr':corr, \n",
        "          'minmax':minmax})"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_MAAa_QD3aJ",
        "outputId": "256f6594-df8c-4767-8fab-30fa05889f31"
      },
      "source": [
        "forecast_accuracy(predicted, actual)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/stattools.py:541: FutureWarning: fft=True will become the default in a future version of statsmodels. To suppress this warning, explicitly set fft=False.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acf1': -0.15685785536159605,\n",
              " 'corr': 0.9897750981698683,\n",
              " 'mae': 0.19000000000000006,\n",
              " 'mape': 0.061415534656913975,\n",
              " 'me': -0.07000000000000009,\n",
              " 'minmax': 0.05995269194407116,\n",
              " 'mpe': -0.015354928596307932,\n",
              " 'mse': 0.04500000000000004,\n",
              " 'rmse': 0.21213203435596434}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV2JNqcXE5Ha"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}